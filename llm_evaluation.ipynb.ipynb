{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1175a638f9544e49bab344099b5db481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe74e835070f4129a8510e87cec52741",
              "IPY_MODEL_fbea272eb3d84faaa4d714797823222e",
              "IPY_MODEL_76903445cdd141e7af775317cd7a78b7"
            ],
            "layout": "IPY_MODEL_74bd96a542e346d6b003273786599e7b"
          }
        },
        "fe74e835070f4129a8510e87cec52741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d5c526a8394b7b90b9dc855ce4dd02",
            "placeholder": "​",
            "style": "IPY_MODEL_dc40ed7916dc4d6eb74f4cd0e272d1d7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "fbea272eb3d84faaa4d714797823222e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91a8823260784088b2e6052d9116b015",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21db5a3df9bb43d8958dccdba9eaed00",
            "value": 2
          }
        },
        "76903445cdd141e7af775317cd7a78b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3dd5a7b25214cda98619d44dde2691f",
            "placeholder": "​",
            "style": "IPY_MODEL_44d2c8bb472d4feb9776396e8d26857e",
            "value": " 2/2 [01:22&lt;00:00, 38.26s/it]"
          }
        },
        "74bd96a542e346d6b003273786599e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41d5c526a8394b7b90b9dc855ce4dd02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc40ed7916dc4d6eb74f4cd0e272d1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91a8823260784088b2e6052d9116b015": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21db5a3df9bb43d8958dccdba9eaed00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3dd5a7b25214cda98619d44dde2691f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44d2c8bb472d4feb9776396e8d26857e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKp9CS6ZAwRA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "86P-YkYqa5Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations and imports"
      ],
      "metadata": {
        "id": "MDhDLzJPAl-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"torch==2.1.2\" tensorboard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bg2hfA_BA_Dc",
        "outputId": "200fd3a9-c8e5-4dc1-9196-065dbb05c1a6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.15.0 requires tensorboard<2.16,>=2.15, but you have tensorboard 2.16.2 which is incompatible.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U \"transformers==4.36.2\" \"datasets==2.16.1\" \"accelerate==0.26.1\" \"bitsandbytes==0.42.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAMg_rljBYFP",
        "outputId": "761e7617-a8bf-4b1f-b3ab-d1f17d791323"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U git+https://github.com/huggingface/trl@a3c5b7178ac4f65569975efadc97db2f3749c65e\n",
        "!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS-m864VDnZF",
        "outputId": "03e08438-92b7-49fb-c742-3ea5dbf2b790"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.0/102.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for trl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ],
      "metadata": {
        "id": "driZvrhvDugy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "zoyetbwYDwbI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, PeftConfig\n",
        "from trl import SFTTrainer\n",
        "from trl import setup_chat_format\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          AutoTokenizer,\n",
        "                          BitsAndBytesConfig,\n",
        "                          TrainingArguments,\n",
        "                          pipeline,\n",
        "                          logging)\n",
        "from sklearn.metrics import (accuracy_score,\n",
        "                             classification_report,\n",
        "                             confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "UwSxogBwD5dG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"pytorch version {torch.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyaaqaCpEGFX",
        "outputId": "a962fab1-677f-4b00-c38d-976862672f32"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch version 2.1.2+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"working on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmSTN550EIPv",
        "outputId": "a09e07e7-4d4d-48ac-85e2-1b2c1babff21"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "working on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preperation"
      ],
      "metadata": {
        "id": "2TORvAFAmzQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preperations\n",
        "\n",
        "filename = \"/IMDB_Dataset.csv\"\n",
        "\n",
        "\n",
        "# df = pd.read_csv(filename,names=[\"reviews\", \"sentiment\"],encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "\n",
        "df = pd.read_csv(filename, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
        "\n",
        "# data = pd.read_csv(\"IMDB_Dataset.csv\")\n",
        "sentiment_counts = df['sentiment'].value_counts()\n",
        "\n",
        "# Print the counts\n",
        "print(\"Sentiment counts:\")\n",
        "print(sentiment_counts)\n",
        "\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "# Split the dataset into train, test, and evaluation sets\n",
        "X_train = list()\n",
        "X_test = list()\n",
        "y_test = list()\n",
        "for sentiment in [\"positive\", \"negative\"]:  # Iterate over sentiment categories\n",
        "    sentiment_df = df[df.sentiment == sentiment]\n",
        "    print(f\"Number of samples for sentiment category '{sentiment}': {len(sentiment_df)}\")  # Print length of DataFrame\n",
        "\n",
        "    if len(sentiment_df) < 2:\n",
        "        raise ValueError(f\"Not enough samples for sentiment category '{sentiment}'\")\n",
        "\n",
        "    train, test = train_test_split(sentiment_df,\n",
        "                                   test_size=0.05,  # 5% of samples for testing\n",
        "                                   random_state=42)\n",
        "    X_train.append(train)\n",
        "    X_test.append(test)\n",
        "    y_test.append(test[\"sentiment\"])\n",
        "\n",
        "X_train = pd.concat(X_train).sample(frac=1, random_state=10)  # Shuffle the DataFrame\n",
        "X_test = pd.concat(X_test)\n",
        "\n",
        "\n",
        "\n",
        "# eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]  # Get evaluation indices\n",
        "# X_eval = df[df.index.isin(eval_idx)]  # Get evaluation DataFrame\n",
        "# X_eval = (X_eval.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(n=50, random_state=10, replace=True)))  # Sample 50 samples from each sentiment category\n",
        "# X_train = X_train.reset_index(drop=True)  # Reset index of training DataFrame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIFtAyexEJaH",
        "outputId": "30ef623a-e362-4dd8-b1f7-1045ec5a94b5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment counts:\n",
            "sentiment\n",
            "positive    25000\n",
            "negative    25000\n",
            "Name: count, dtype: int64\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "Number of samples for sentiment category 'positive': 25000\n",
            "Number of samples for sentiment category 'negative': 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]  # Get evaluation indices\n",
        "X_eval = df[df.index.isin(eval_idx)]  # Get evaluation DataFrame\n",
        "X_eval = (X_eval.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(n=50, random_state=10, replace=True)))  # Sample 50 samples from each sentiment category\n",
        "X_train = X_train.reset_index(drop=True)  # Reset index of training DataFrame"
      ],
      "metadata": {
        "id": "qx7HBZrIkBdm"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iul8UEPGjsQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test)"
      ],
      "metadata": {
        "id": "BgXgImrSf4u3",
        "outputId": "787cde63-9ec0-41c7-b8b3-68974631a3f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  review sentiment\n",
            "13886  I don't know how or why this film has a meager...  positive\n",
            "48027  For a long time it seemed like all the good Ca...  positive\n",
            "19536  Terry Gilliam's and David Peoples' teamed up t...  positive\n",
            "27232  What is there to say about an anti-establishme...  positive\n",
            "28001  This movie was made only 48 years after the en...  positive\n",
            "...                                                  ...       ...\n",
            "14279  Dr. Seuss would sure be mad right now if he wa...  negative\n",
            "14181  OK, so I gotta start this review by saying i w...  negative\n",
            "28045  If you're watching this without an inkling of ...  negative\n",
            "41296  This movie truly shows the farce and hypocrisy...  negative\n",
            "31710  This movie sucks so bad. Its funny to see what...  negative\n",
            "\n",
            "[2500 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Analyze the sentiment of the movie review,\n",
        "            determine if it is positive, or negative, and return the answer as\n",
        "            the corresponding sentiment label \"positive\" or \"negative\".\n",
        "\n",
        "            [{data_point[\"review\"]}] = {data_point[\"sentiment\"]}\n",
        "            \"\"\".strip()\n",
        "\n",
        "def generate_test_prompt(data_point):\n",
        "    return f\"\"\"\n",
        "            Analyze the sentiment of the movie,\n",
        "            determine if it is positive, or negative, and return the answer as\n",
        "            the corresponding sentiment label \"positive\" or \"negative\".\n",
        "\n",
        "            [{data_point[\"review\"]}] = \"\"\".strip()\n",
        "\n",
        "X_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), columns=[\"review\"])\n",
        "X_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), columns=[\"review\"])\n",
        "\n",
        "y_true = X_test.sentiment\n",
        "X_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"review\"])\n",
        "\n",
        "train_data = Dataset.from_pandas(X_train)\n",
        "eval_data = Dataset.from_pandas(X_eval)"
      ],
      "metadata": {
        "id": "qFEpJxdJE4zb"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a function to evaluate the results from the fine-tuned sentiment model."
      ],
      "metadata": {
        "id": "etLsHzHTAl-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(y_true, y_pred):\n",
        "\n",
        "    mapping = {'positive': 1, 'negative': 0}\n",
        "    def map_func(x):\n",
        "        return mapping.get(x, 1)\n",
        "\n",
        "    y_true = np.vectorize(map_func)(y_true)\n",
        "    y_pred = np.vectorize(map_func)(y_pred)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
        "    print(f'Accuracy: {accuracy:.3f}')\n",
        "\n",
        "    # Generate accuracy report\n",
        "    unique_labels = set(y_true)  # Get unique labels\n",
        "\n",
        "    for label in unique_labels:\n",
        "        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]\n",
        "        label_y_true = [y_true[i] for i in label_indices]\n",
        "        label_y_pred = [y_pred[i] for i in label_indices]\n",
        "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
        "        print(f'Accuracy for label {label}: {accuracy:.3f}')\n",
        "\n",
        "    # Generate classification report\n",
        "    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n",
        "    print('\\nClassification Report:')\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n",
        "    print('\\nConfusion Matrix:')\n",
        "    print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "A8T8R-HsElGY"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
        "\n",
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",device_map=device,\n",
        "#     torch_dtype=compute_dtype,\n",
        "#     quantization_config=bnb_config,)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'mistralai/Mistral-7B-v0.1',\n",
        "    device_map=device,\n",
        "    torch_dtype=compute_dtype,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1',\n",
        "                                          trust_remote_code=True,\n",
        "                                         )\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1175a638f9544e49bab344099b5db481",
            "fe74e835070f4129a8510e87cec52741",
            "fbea272eb3d84faaa4d714797823222e",
            "76903445cdd141e7af775317cd7a78b7",
            "74bd96a542e346d6b003273786599e7b",
            "41d5c526a8394b7b90b9dc855ce4dd02",
            "dc40ed7916dc4d6eb74f4cd0e272d1d7",
            "91a8823260784088b2e6052d9116b015",
            "21db5a3df9bb43d8958dccdba9eaed00",
            "a3dd5a7b25214cda98619d44dde2691f",
            "44d2c8bb472d4feb9776396e8d26857e"
          ]
        },
        "id": "QwBG3LklFus8",
        "outputId": "4df1fe1a-e96f-4822-a7cc-d6851fa20a84"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1175a638f9544e49bab344099b5db481"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test, model, tokenizer):\n",
        "    y_pred = []\n",
        "    for i in tqdm(range(len(X_test))):\n",
        "        prompt = X_test.iloc[i][\"review\"]\n",
        "        pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 1,\n",
        "                        temperature = 0.0,\n",
        "                       )\n",
        "        result = pipe(prompt)\n",
        "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "        if \"positive\" in answer:\n",
        "            y_pred.append(\"positive\")\n",
        "        elif \"negative\" in answer:\n",
        "            y_pred.append(\"negative\")\n",
        "\n",
        "        else:\n",
        "            y_pred.append(\"none\")\n",
        "    return y_pred\n",
        "\n",
        "y_pred = predict(test, model, tokenizer)\n",
        "\n",
        "\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "id": "bxPhEWxSmGHU",
        "outputId": "91776d49-367f-4b6a-dc8f-3fa0f1f161f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [21:56<00:00,  1.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.654\n",
            "Accuracy for label 0: 0.325\n",
            "Accuracy for label 1: 0.982\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.32      0.48      1250\n",
            "           1       0.59      0.98      0.74      1250\n",
            "\n",
            "    accuracy                           0.65      2500\n",
            "   macro avg       0.77      0.65      0.61      2500\n",
            "weighted avg       0.77      0.65      0.61      2500\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 406  844    0]\n",
            " [  22 1228    0]\n",
            " [   0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test, model, tokenizer):\n",
        "    y_pred = []\n",
        "    for i in tqdm(range(len(X_test))):\n",
        "        prompt = X_test.iloc[i][\"review\"]\n",
        "        pipe = pipeline(task=\"text-generation\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        max_new_tokens = 1,\n",
        "                        temperature = 0.0,\n",
        "                       )\n",
        "        result = pipe(prompt)\n",
        "        answer = result[0]['generated_text'].split(\"=\")[-1]\n",
        "        if \"positive\" in answer:\n",
        "            y_pred.append(\"positive\")\n",
        "        elif \"negative\" in answer:\n",
        "            y_pred.append(\"negative\")\n",
        "\n",
        "        else:\n",
        "            y_pred.append(\"none\")\n",
        "    return y_pred\n",
        "\n",
        "y_pred = predict(test, model, tokenizer)\n",
        "\n",
        "\n",
        "evaluate(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W05ahGwQOG4A",
        "outputId": "bbc5e130-ac19-4a5a-a8b4-51295a84cc07"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2500/2500 [22:12<00:00,  1.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.833\n",
            "Accuracy for label 0: 0.880\n",
            "Accuracy for label 1: 0.786\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.88      0.84      1250\n",
            "           1       0.87      0.79      0.82      1250\n",
            "\n",
            "    accuracy                           0.83      2500\n",
            "   macro avg       0.84      0.83      0.83      2500\n",
            "weighted avg       0.84      0.83      0.83      2500\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1100  150    0]\n",
            " [ 268  982    0]\n",
            " [   0    0    0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}